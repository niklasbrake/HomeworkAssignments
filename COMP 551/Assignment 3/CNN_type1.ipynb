{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Activation\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.convolutional import AveragePooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.utils import plot_model\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "x = np.loadtxt(\"train_x.csv\", delimiter=\",\")  # load from text\n",
    "y = np.loadtxt(\"train_y.csv\", delimiter=\",\")\n",
    "z = np.loadtxt(\"test_x.csv\", delimiter=\",\")\n",
    "x = x.reshape(-1, 64 * 64).astype('float32')  # reshape\n",
    "y = y.reshape(-1).astype('float32')\n",
    "z = z.reshape(-1, 64 * 64).astype('float32')\n",
    "x_test = x[:220]\n",
    "y_test = y [:220]\n",
    "x_train = x[:-5000]\n",
    "y_train = y[:-5000]\n",
    "    #validate set\n",
    "x_validate = x[-5000:]\n",
    "y_validate = y[-5000:]\n",
    "unique_classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 24, 25, 27, 28, 30, 32, 35, 36, 40, 42, 45, 48, 49, 54, 56, 63, 64, 72, 81]\n",
    "\n",
    "def one_hot_encoding(y):\n",
    "    encoded_y = []\n",
    "    N = len(y)\n",
    "    for i in range(N):\n",
    "        encoded_y.append(unique_classes.index(int(y[i])))\n",
    "    return encoded_y\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 3\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load data\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = x_train.reshape(-1, 1, 64, 64).astype('float32')\n",
    "X_test = x_validate.reshape(-1, 1, 64, 64).astype('float32')\n",
    "\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(one_hot_encoding(y_train))\n",
    "y_test = np_utils.to_categorical(one_hot_encoding(y_validate))\n",
    "num_classes = y_test.shape[1]\n",
    "\n",
    "# define the larger model\n",
    "def larger_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(120, (5, 5),strides=2, input_shape=(1, 64, 64), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(60, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(30, (2, 2), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = larger_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"./model/weights-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=2, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "history=model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=100,verbose=2,callbacks=callbacks_list)#,callbacks=callbacks_list\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "prediction = model.predict_classes(X_test.astype('float32'))\n",
    "print(prediction)\n",
    "prediction = model.predict_classes(z.reshape(-1, 1, 64, 64).astype('float32'))\n",
    "with open ('result.txt','w') as result:\n",
    "    result.write(\"Id,Label\\n\")\n",
    "    id = 1\n",
    "    for i in prediction:\n",
    "        result.write(str(id) + \",\" + str(i) + \"\\n\")\n",
    "        id += 1\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))\n",
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g^')\n",
    "plt.plot(epochs, val_loss_values, 'bs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
